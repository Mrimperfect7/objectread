<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Vision Assistant (Real Detection)</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
  <style>
    body {
      font-family: sans-serif;
      text-align: center;
      background: #f3f4f6;
      padding: 20px;
    }
    video {
      width: 100%;
      max-width: 500px;
      border-radius: 12px;
    }
    canvas {
      position: absolute;
      top: 0;
      left: 0;
    }
    #log {
      margin-top: 20px;
      font-size: 18px;
      color: #111827;
    }
    #container {
      position: relative;
      display: inline-block;
    }
  </style>
</head>
<body>
  <h1>ðŸŽ¯ Vision Assistant</h1>
  <p>Real-time object detection + voice for the visually impaired</p>

  <div id="container">
    <video id="video" autoplay playsinline muted></video>
    <canvas id="canvas"></canvas>
  </div>

  <p id="log">Loading model...</p>

  <script>
    const video = document.getElementById("video");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");
    const log = document.getElementById("log");
    let model;
    let lastSpoken = "";

    // Load COCO-SSD model
    cocoSsd.load().then(loadedModel => {
      model = loadedModel;
      log.textContent = "Model loaded. Starting camera...";
      startCamera();
    });

    function startCamera() {
      navigator.mediaDevices.getUserMedia({
        video: {
          facingMode: { exact: "environment" } // Back camera
        },
        audio: false
      }).then(stream => {
        video.srcObject = stream;
        video.onloadedmetadata = () => {
          video.play();
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          detectFrame();
        };
      }).catch(err => {
        log.textContent = "Camera access error: " + err;
      });
    }

    function speak(text) {
      if ('speechSynthesis' in window) {
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = 'en-US';
        utterance.rate = 1;
        speechSynthesis.speak(utterance);
      }
    }

    function detectFrame() {
      model.detect(video).then(predictions => {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        const detectedLabels = [];

        predictions.forEach(prediction => {
          const [x, y, width, height] = prediction.bbox;
          const label = prediction.class;

          // Draw box
          ctx.strokeStyle = "#00FF00";
          ctx.lineWidth = 2;
          ctx.strokeRect(x, y, width, height);

          // Draw label
          ctx.fillStyle = "#00FF00";
          ctx.font = "16px Arial";
          ctx.fillText(label, x, y > 10 ? y - 5 : 10);

          detectedLabels.push(label);
        });

        // Speak only if new object is detected
        const uniqueObjects = [...new Set(detectedLabels)];
        const currentLabel = uniqueObjects.join(', ');

        if (currentLabel && currentLabel !== lastSpoken) {
          lastSpoken = currentLabel;
          speak(`I see ${currentLabel}`);
          log.textContent = `Detected: ${currentLabel}`;
        }

        requestAnimationFrame(detectFrame);
      });
    }
  </script>
</body>
</html>
